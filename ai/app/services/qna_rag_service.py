"""
Q&A PDF ë¬¸ì„œë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œ

LlamaParseì™€ LlamaIndexë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸-ë‹µë³€ í˜•ì‹ì˜ PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³ 
íš¨ìœ¨ì ì¸ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì„¤ê³„ ì›ì¹™:
- ê° PDFì˜ ì²­í¬ëŠ” document_chunks í…Œì´ë¸”ì— ì €ì¥ë¨ (pgvector ì‚¬ìš©)
- PDFê°€ í´ë”ë¥¼ ë³€ê²½í•´ë„ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (document_idë¡œ ì—°ê²°)
- ì¿¼ë¦¬ ì‹œ í´ë”ì˜ PDFë“¤ì„ ê°ê° ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ í†µí•©
- ì¬ì¸ë±ì‹± ì—†ìŒ: í•œë²ˆ ì¸ë±ì‹±ëœ PDFëŠ” DBì— ì˜êµ¬ ì €ì¥
"""

import os
import uuid
import re
import json
from pathlib import Path
from typing import Optional, List, Dict
from llama_index.core import (
    Settings,
    Document,
)
from llama_index.core.node_parser import MarkdownElementNodeParser
from llama_index.core.schema import TextNode
from llama_index.core.prompts import PromptTemplate
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_parse import LlamaParse
from app.core.database import Database


class QnARAGService:
    """Q&A PDF ë¬¸ì„œë¥¼ ìœ„í•œ RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(
        self,
        openai_api_key: str,
        llama_cloud_api_key: str,
    ):
        """
        QnARAGService ì´ˆê¸°í™”

        Args:
            openai_api_key: OpenAI API í‚¤
            llama_cloud_api_key: LlamaCloud API í‚¤ (LlamaParse ì‚¬ìš©)
        """
        # OpenAI LLM ë° ì„ë² ë”© ì„¤ì •
        Settings.llm = OpenAI(
            model="gpt-4o-mini",
            api_key=openai_api_key,
            temperature=0.1,
        )
        Settings.embed_model = OpenAIEmbedding(
            model_name="text-embedding-3-small",
            api_key=openai_api_key,
        )

        # LlamaParse ì´ˆê¸°í™”
        self.parser = LlamaParse(
            api_key=llama_cloud_api_key,
            result_type="markdown",  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
            num_workers=4,  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
            verbose=True,
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì €ì¥ (ì²­í¬ ì €ì¥ ì‹œ ì‚¬ìš©)
        self.embed_model = Settings.embed_model
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ (êµ¬ì¡°í™” íŒŒì‹±ìš©)
        self.llm = Settings.llm

    def _parse_qna_pairs_with_llm(self, text: str) -> List[Dict[str, str]]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸-ë‹µë³€ ìŒì„ êµ¬ì¡°í™”ëœ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œ
        
        ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ íŒŒì‹±í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ í˜•ì‹ë„ ì²˜ë¦¬ ê°€ëŠ¥
        
        Args:
            text: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            
        Returns:
            ì§ˆë¬¸-ë‹µë³€ ìŒ ë¦¬ìŠ¤íŠ¸ [{"question": "...", "answer": "..."}, ...]
        """
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤ë©´, ê° ìŒì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text[:8000]}  # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì²˜ë¦¬

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "qa_pairs": [
    {{"question": "ì§ˆë¬¸ ë‚´ìš©", "answer": "ë‹µë³€ ë‚´ìš©"}},
    ...
  ]
}}

ì§ˆë¬¸-ë‹µë³€ ìŒì´ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ (êµ¬ì¡°í™”ëœ ì¶œë ¥)
            response = self.llm.complete(prompt)
            response_text = str(response).strip()
            
            # JSON ì¶”ì¶œ ì‹œë„
            # ì‘ë‹µì´ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì ¸ ìˆì„ ìˆ˜ ìˆìŒ
            if "```json" in response_text:
                response_text = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if response_text:
                    response_text = response_text.group(1)
            elif "```" in response_text:
                response_text = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
                if response_text:
                    response_text = response_text.group(1)
            
            # JSON íŒŒì‹±
            try:
                result = json.loads(response_text)
                qa_pairs = result.get("qa_pairs", [])
                
                # ìœ íš¨ì„± ê²€ì‚¬
                valid_pairs = []
                for pair in qa_pairs:
                    if isinstance(pair, dict) and "question" in pair and "answer" in pair:
                        question = str(pair["question"]).strip()
                        answer = str(pair["answer"]).strip()
                        if question and answer:
                            valid_pairs.append({"question": question, "answer": answer})
                
                if valid_pairs:
                    print(f"LLMìœ¼ë¡œ {len(valid_pairs)}ê°œì˜ Q&A ìŒ ì¶”ì¶œ ì„±ê³µ")
                    return valid_pairs
            except json.JSONDecodeError as e:
                print(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}")
        
        except Exception as e:
            print(f"LLM ê¸°ë°˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return []

    def _parse_qna_pairs_with_markdown_structure(self, text: str) -> List[Dict[str, str]]:
        """
        ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸-ë‹µë³€ ìŒ ì¶”ì¶œ
        
        í—¤ë”, ë¦¬ìŠ¤íŠ¸, ë¸”ë¡ ì¸ìš© ë“±ì˜ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ í™œìš©
        
        Args:
            text: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            
        Returns:
            ì§ˆë¬¸-ë‹µë³€ ìŒ ë¦¬ìŠ¤íŠ¸
        """
        qna_pairs = []
        
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í•  (# ì§ˆë¬¸, ## ë‹µë³€ ë“±)
        # í—¤ë” íŒ¨í„´: # ì§ˆë¬¸, ## ë‹µë³€, ### ì§ˆë¬¸ ë“±
        header_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
        sections = []
        current_section = {"level": 0, "title": "", "content": []}
        
        lines = text.split('\n')
        for line in lines:
            header_match = header_pattern.match(line)
            if header_match:
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section["title"]:
                    sections.append(current_section.copy())
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = {
                    "level": len(header_match.group(1)),
                    "title": header_match.group(2).strip(),
                    "content": []
                }
            else:
                if line.strip():
                    current_section["content"].append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section["title"]:
            sections.append(current_section)
        
        # ì„¹ì…˜ë“¤ì„ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ ë§¤ì¹­
        # ì¸ì ‘í•œ ì„¹ì…˜ì´ ì§ˆë¬¸/ë‹µë³€ì¼ ê°€ëŠ¥ì„± ì²´í¬
        for i in range(len(sections) - 1):
            section1 = sections[i]
            section2 = sections[i + 1]
            
            title1 = section1["title"].lower()
            title2 = section2["title"].lower()
            content1 = '\n'.join(section1["content"]).strip()
            content2 = '\n'.join(section2["content"]).strip()
            
            # ì§ˆë¬¸/ë‹µë³€ íŒ¨í„´ ê°ì§€
            is_qna = False
            question = ""
            answer = ""
            
            # íŒ¨í„´ 1: ì œëª©ì— "ì§ˆë¬¸", "ë‹µë³€" í‚¤ì›Œë“œ
            if any(keyword in title1 for keyword in ["ì§ˆë¬¸", "question", "q"]) and \
               any(keyword in title2 for keyword in ["ë‹µë³€", "answer", "a"]):
                question = content1 if content1 else section1["title"]
                answer = content2 if content2 else section2["title"]
                is_qna = True
            # íŒ¨í„´ 2: ì²« ë²ˆì§¸ ì„¹ì…˜ì´ ì§ˆë¬¸ì²˜ëŸ¼ ë³´ì´ê³  ë‘ ë²ˆì§¸ê°€ ë‹µë³€
            elif "?" in title1 or "?" in content1[:100]:
                question = content1 if content1 else section1["title"]
                answer = content2 if content2 else section2["title"]
                is_qna = True
            
            if is_qna and question and answer:
                qna_pairs.append({"question": question, "answer": answer})
        
        # ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ Q&A ì¶”ì¶œ
        # ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸-ë‹µë³€ ì¶”ì¶œ
        list_pattern = re.compile(r'^\s*(\d+)[\.\)]\s*(.+?)(?=\n\s*\d+[\.\)]|$)', re.MULTILINE | re.DOTALL)
        list_items = list_pattern.findall(text)
        
        # ì§ìˆ˜ ê°œì˜ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ ë§¤ì¹­
        for i in range(0, len(list_items) - 1, 2):
            question = list_items[i][1].strip()
            answer = list_items[i + 1][1].strip()
            if question and answer and len(question) < 500:  # ì§ˆë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì œì™¸
                qna_pairs.append({"question": question, "answer": answer})
        
        return qna_pairs

    def _parse_qna_pairs_from_text(self, text: str, use_llm: bool = True) -> List[Dict[str, str]]:
        """
        ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ì¶”ì¶œ
        
        ì—¬ëŸ¬ ë°©ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„:
        1. ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„
        2. LLM ê¸°ë°˜ êµ¬ì¡°í™” íŒŒì‹± (ì •í™•í•˜ì§€ë§Œ ëŠë¦¬ê³  ë¹„ìš© ë°œìƒ)
        
        Args:
            text: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            use_llm: LLM ê¸°ë°˜ íŒŒì‹± ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            ì§ˆë¬¸-ë‹µë³€ ìŒ ë¦¬ìŠ¤íŠ¸ [{"question": "...", "answer": "..."}, ...]
        """
        qna_pairs = []
        
        # ë°©ë²• 1: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ì‹œë„
        qna_pairs = self._parse_qna_pairs_with_markdown_structure(text)
        
        # ë°©ë²• 2: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ì´ ì‹¤íŒ¨í•˜ë©´ LLM ì‚¬ìš© (ì˜µì…˜)
        if not qna_pairs and use_llm:
            print("ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨, LLM ê¸°ë°˜ íŒŒì‹± ì‹œë„...")
            llm_pairs = self._parse_qna_pairs_with_llm(text)
            if llm_pairs:
                qna_pairs = llm_pairs
        
        return qna_pairs

    def _create_qna_nodes_from_documents(
        self,
        documents: List[Document],
        document_id: str,
        pdf_path: str,
    ) -> List[TextNode]:
        """
        Document ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ì¶”ì¶œí•˜ì—¬ TextNode ë¦¬ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            documents: LlamaParseë¡œ íŒŒì‹±ëœ Document ë¦¬ìŠ¤íŠ¸
            document_id: ë¬¸ì„œ ID
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ êµ¬ì„±ëœ TextNode ë¦¬ìŠ¤íŠ¸
        """
        all_nodes = []
        pdf_name = os.path.basename(pdf_path)
        
        for doc_idx, doc in enumerate(documents):
            doc_text = doc.text if hasattr(doc, 'text') else str(doc)
            
            # ì§ˆë¬¸-ë‹µë³€ ìŒ ì¶”ì¶œ (LLM ì‚¬ìš© ì˜µì…˜ í¬í•¨)
            # í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ LLM ì‚¬ìš© ì•ˆ í•¨ (ë¹„ìš© ì ˆê°)
            use_llm = len(doc_text) > 500 and len(doc_text) < 10000  # ì ë‹¹í•œ ê¸¸ì´ì¼ ë•Œë§Œ
            qna_pairs = self._parse_qna_pairs_from_text(doc_text, use_llm=use_llm)
            
            if qna_pairs:
                print(f"ë¬¸ì„œ {doc_idx + 1}ì—ì„œ {len(qna_pairs)}ê°œì˜ Q&A ìŒ ì¶”ì¶œ")
                
                for qna_idx, qna_pair in enumerate(qna_pairs):
                    # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
                    combined_text = f"{qna_pair['question']}\n\n{qna_pair['answer']}"
                    
                    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    node_metadata = {
                        'document_id': document_id,
                        'pdf_path': pdf_path,
                        'pdf_name': pdf_name,
                        'qna_index': qna_idx + 1,
                        'question': qna_pair['question'],
                        'answer': qna_pair['answer'],
                        'chunk_type': 'qna_pair',
                    }
                    
                    # ì›ë³¸ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ë³‘í•©
                    if hasattr(doc, 'metadata') and doc.metadata:
                        node_metadata.update({
                            k: v for k, v in doc.metadata.items() 
                            if k not in ['document_id', 'pdf_path', 'pdf_name']
                        })
                    
                    # TextNode ìƒì„±
                    node = TextNode(
                        text=combined_text,
                        metadata=node_metadata,
                    )
                    all_nodes.append(node)
            else:
                # Q&A ìŒì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # (MarkdownElementNodeParserë¡œ í´ë°±)
                print(f"ë¬¸ì„œ {doc_idx + 1}ì—ì„œ Q&A ìŒì„ ì°¾ì§€ ëª»í•¨. ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                node_parser = MarkdownElementNodeParser()
                fallback_nodes = node_parser.get_nodes_from_documents([doc])
                
                for node in fallback_nodes:
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if not hasattr(node, 'metadata') or node.metadata is None:
                        node.metadata = {}
                    node.metadata['document_id'] = document_id
                    node.metadata['pdf_path'] = pdf_path
                    node.metadata['pdf_name'] = pdf_name
                    node.metadata['chunk_type'] = 'fallback'
                
                all_nodes.extend(fallback_nodes)
        
        return all_nodes

    def _parse_pdf(self, pdf_path: str) -> List[Document]:
        """
        PDF íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            íŒŒì‹±ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        print(f"PDF íŒŒì‹± ì‹œì‘: {pdf_path}")

        # LlamaParseë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì‹±
        documents = self.parser.load_data(pdf_path)

        print(f"íŒŒì‹± ì™„ë£Œ: {len(documents)}ê°œì˜ ë¬¸ì„œ ìƒì„±")
        return documents

    def build_index_for_document(
        self,
        document_id: str,
        pdf_path: str,
        folder_id: Optional[str] = None,  # í´ë” ì •ë³´ëŠ” ë©”íƒ€ë°ì´í„°ì—ë§Œ ì €ì¥
    ) -> bool:
        """
        íŠ¹ì • PDF ë¬¸ì„œì— ëŒ€í•œ ì¸ë±ìŠ¤ êµ¬ì¶• (document_chunks í…Œì´ë¸”ì— ì €ì¥)

        Args:
            document_id: ë¬¸ì„œ ID (UUID)
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            folder_id: í´ë” ID (ë©”íƒ€ë°ì´í„°ìš©, ì¸ë±ìŠ¤ êµ¬ì¡°ì—ëŠ” ì˜í–¥ ì—†ìŒ)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ê¸°ì¡´ ì²­í¬ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì¬ì¸ë±ì‹± ë°©ì§€)
            db = Database.get_client()
            existing_chunks = (
                db.table("document_chunks")
                .select("id", count="exact")
                .eq("document_id", document_id)
                .execute()
            )
            
            if existing_chunks.count > 0:
                print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. document_id={document_id}, ì²­í¬ ìˆ˜={existing_chunks.count}")
                return True
            
            # PDF íŒŒì‹±
            documents = self._parse_pdf(pdf_path)
            
            # ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ ë…¸ë“œ ìƒì„±
            all_nodes = self._create_qna_nodes_from_documents(
                documents=documents,
                document_id=document_id,
                pdf_path=pdf_path,
            )
            
            print(f"ì´ {len(all_nodes)}ê°œì˜ ë…¸ë“œ ìƒì„±")
            
            # ì§ˆì˜ì‘ë‹µìŒì´ ìˆëŠ” ë…¸ë“œë§Œ í•„í„°ë§ (chunk_typeì´ 'qna_pair'ì¸ ë…¸ë“œë§Œ)
            qna_nodes = [
                node for node in all_nodes 
                if hasattr(node, 'metadata') and node.metadata and node.metadata.get('chunk_type') == 'qna_pair'
            ]
            
            print(f"ì§ˆì˜ì‘ë‹µìŒì´ ìˆëŠ” ë…¸ë“œ: {len(qna_nodes)}ê°œ (ì „ì²´ {len(all_nodes)}ê°œ ì¤‘)")
            
            if len(qna_nodes) == 0:
                print("ê²½ê³ : ì§ˆì˜ì‘ë‹µìŒì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. DBì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
            
            # ê° ë…¸ë“œë¥¼ document_chunks í…Œì´ë¸”ì— ì €ì¥
            db = Database.get_client()
            saved_count = 0
            
            for node in qna_nodes:
                try:
                    # ë…¸ë“œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                    embedding = self.embed_model.get_text_embedding(node.text)
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    node_metadata = node.metadata if hasattr(node, 'metadata') and node.metadata else {}
                    
                    # RPC í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ document_chunks í…Œì´ë¸”ì— ì €ì¥
                    # Supabase REST APIëŠ” VECTOR íƒ€ì…ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ RPC í•¨ìˆ˜ ì‚¬ìš©
                    chunk_metadata = {
                        "pdf_name": node_metadata.get('pdf_name', os.path.basename(pdf_path)),
                        "pdf_path": node_metadata.get('pdf_path', pdf_path),
                        **{k: v for k, v in node_metadata.items() if k not in ['pdf_name', 'pdf_path']},
                    }
                    
                    # RPC í•¨ìˆ˜ í˜¸ì¶œ
                    try:
                        result = db.rpc(
                            "insert_document_chunk",
                            {
                                "p_document_id": document_id,
                                "p_content": node.text,
                                "p_embedding": embedding,  # VECTOR íƒ€ì…ìœ¼ë¡œ ì €ì¥
                                "p_metadata": chunk_metadata,
                            }
                        ).execute()
                        
                        if result.data:
                            saved_count += 1
                    except Exception as rpc_error:
                        # RPC í•¨ìˆ˜ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°, ì§ì ‘ insert ì‹œë„
                        print(f"RPC í•¨ìˆ˜ í˜¸ì¶œ ì‹¤íŒ¨, ì§ì ‘ insert ì‹œë„: {rpc_error}")
                        try:
                            # SupabaseëŠ” VECTOR íƒ€ì…ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
                            # ì„ë² ë”©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ë‚˜ì¤‘ì— RPC í•¨ìˆ˜ë¡œ ë³€í™˜ í•„ìš”)
                            # ë˜ëŠ” psycopg2ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì €ì¥
                            import json
                            chunk_data = {
                                "document_id": document_id,
                                "content": node.text,
                                "embedding": json.dumps(embedding),  # ì„ì‹œë¡œ JSON ë¬¸ìì—´ë¡œ ì €ì¥
                                "metadata": chunk_metadata,
                            }
                            result = db.table("document_chunks").insert(chunk_data).execute()
                            if result.data:
                                saved_count += 1
                                print(f"ì§ì ‘ insert ì„±ê³µ (ì„ì‹œ)")
                        except Exception as insert_error:
                            print(f"ì§ì ‘ insertë„ ì‹¤íŒ¨: {insert_error}")
                            continue
                    
                except Exception as e:
                    print(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(f"document_chunks í…Œì´ë¸”ì— {saved_count}/{len(qna_nodes)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            
            if saved_count > 0:
                return True
            else:
                print("ê²½ê³ : ì €ì¥ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def remove_document_index(self, document_id: str) -> bool:
        """
        íŠ¹ì • PDFì˜ ì¸ë±ìŠ¤ ì œê±° (document_chunks í…Œì´ë¸”ì—ì„œ ì‚­ì œ)

        Args:
            document_id: ë¬¸ì„œ ID

        Returns:
            ì œê±° ì„±ê³µ ì—¬ë¶€
        """
        try:
            # DBì—ì„œ ì²­í¬ ì‚­ì œ (CASCADEë¡œ ìë™ ì‚­ì œë˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ)
            db = Database.get_client()
            result = (
                db.table("document_chunks")
                .delete()
                .eq("document_id", document_id)
                .execute()
            )
            
            print(f"PDF ì¸ë±ìŠ¤ ì œê±° ì™„ë£Œ (DB): {document_id}")
            return True
        except Exception as e:
            print(f"PDF ì¸ë±ìŠ¤ ì œê±° ì‹¤íŒ¨: {e}")
            return False

    def _validate_question(self, question: str) -> bool:
        """
        ì§ˆë¬¸ì´ ì •ìƒì ì¸ì§€ ê²€ì¦
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ì •ìƒì ì¸ ì§ˆë¬¸ì´ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
        """
        if not question or not question.strip():
            return False
        
        # ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° (5ì ë¯¸ë§Œ)
        if len(question.strip()) < 5:
            return False
        
        # íŠ¹ìˆ˜ë¬¸ìë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ì²´í¬
        cleaned_question = question.strip()
        if not cleaned_question or len(cleaned_question.replace(' ', '').replace('?', '').replace('ï¼Ÿ', '')) < 3:
            return False
        
        # LLMìœ¼ë¡œ ì§ˆë¬¸ì´ ì •ìƒì ì¸ì§€ ê²€ì¦
        validation_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ì •ìƒì ì¸ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
ì •ìƒì ì¸ ì§ˆë¬¸ì´ë€ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜, íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ë¬¼ì–´ë³´ëŠ” ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

í…ìŠ¤íŠ¸: "{question}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "is_valid": true ë˜ëŠ” false,
  "reason": "íŒë‹¨ ì´ìœ "
}}"""

        try:
            response = self.llm.complete(validation_prompt)
            response_text = str(response).strip()
            
            # JSON ì¶”ì¶œ
            if "```json" in response_text:
                response_text = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if response_text:
                    response_text = response_text.group(1)
            elif "```" in response_text:
                response_text = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
                if response_text:
                    response_text = response_text.group(1)
            
            result = json.loads(response_text)
            is_valid = result.get("is_valid", False)
            
            if not is_valid:
                print(f"ì§ˆë¬¸ ê²€ì¦ ì‹¤íŒ¨: {result.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìœ ')}")
            
            return is_valid
        except Exception as e:
            print(f"ì§ˆë¬¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê·œì¹™ìœ¼ë¡œ íŒë‹¨
            return len(question.strip()) >= 5

    def query_documents(
        self,
        question: str,
        document_ids: List[str],
        similarity_top_k: int = 3,
    ) -> str:
        """
        ì—¬ëŸ¬ PDF ë¬¸ì„œë“¤ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            document_ids: ê²€ìƒ‰í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            similarity_top_k: ê° ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 3)

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        if not document_ids:
            raise ValueError("ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ì§ˆë¬¸ ê²€ì¦
        if not self._validate_question(question):
            # ì •ìƒì ì´ì§€ ì•Šì€ ì§ˆë¬¸ì¸ ê²½ìš° RAG ì—†ì´ ë°”ë¡œ ë‹µë³€ ìƒì„±
            from datetime import datetime
            current_date = datetime.now().strftime("%Y. %m. %d.")
            
            invalid_question_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë¬¸ì„œ ì‘ì„±ìì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì´ ì •ìƒì ì´ì§€ ì•Šì•„ ë¬¸ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

**ì§ˆë¬¸:**
{question}

**ë³´ê³ ì„œ ì‘ì„± í˜•ì‹:**

ë³´ê³ ì„œ (ì´ˆì•ˆ)
ì¼ì: {current_date}

ì§ˆë¬¸: {question}

info
ì§ˆë¬¸ í™•ì¸ ìš”ì²­

ì…ë ¥í•˜ì‹  ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ êµ¬ì²´ì ì´ì§€ ì•Šì•„ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.

[ìƒì„¸ ë‹µë³€ ë‚´ìš©]

1. ì§ˆë¬¸ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­
ì…ë ¥í•˜ì‹  ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì‹œë©´, ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€í† í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:
- êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ì§€ ëª…í™•íˆ ì‘ì„±í•´ ì£¼ì„¸ìš”
- ê´€ë ¨ëœ ì£¼ì œë‚˜ ë¶„ì•¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•´ ì£¼ì„¸ìš”
- í•„ìš”í•œ ì •ë³´ë‚˜ ê¸°ì¤€, ì ˆì°¨ ë“±ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì‘ì„±í•´ ì£¼ì„¸ìš”

ì˜ˆì‹œ:
- "ì˜ì•½í’ˆ Aì˜ ì‚¬ìš©ë²•ê³¼ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ì‹í’ˆ ì²¨ê°€ë¬¼ Bì˜ ì•ˆì „ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ê³µë™ì£¼íƒ ì¸µê°„ì†ŒìŒ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ê±´ì¶• í—ˆê°€ ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."

2. ì¶”ê°€ ì•ˆë‚´
ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‘ì„±í•´ ì£¼ì‹œë©´, ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€í† í•˜ì—¬ ìƒì„¸í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
"""
            
            print(f"ì§ˆë¬¸ ê²€ì¦ ì‹¤íŒ¨: '{question}' - RAG ì—†ì´ ë‹µë³€ ìƒì„±")
            response = self.llm.complete(invalid_question_prompt)
            answer = str(response).strip() if response else ""
            return answer

        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.embed_model.get_query_embedding(question)
        
        # pgvectorë¥¼ ì‚¬ìš©í•˜ì—¬ document_chunks í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰
        # Supabaseì˜ ê²½ìš° RPC í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ SQL ì¿¼ë¦¬ í•„ìš”
        nodes = self._search_chunks_with_pgvector(
            query_embedding=query_embedding,
            document_ids=document_ids,
            similarity_top_k=similarity_top_k,
        )
        
        if not nodes:
            raise ValueError("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìƒìœ„ kê°œ ë…¸ë“œ ì„ íƒ
        top_nodes = nodes[:similarity_top_k]
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¯¼ì› ë‹µë³€ì„œ í˜•ì‹)
        from datetime import datetime
        current_date = datetime.now().strftime("%Y. %m. %d.")
        
        # RAGì—ì„œ ê²€ìƒ‰ëœ ë…¸ë“œì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
        context_texts = []
        for node in top_nodes:
            content = node.get('content', '')
            if content:
                context_texts.append(content)
        
        # ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        context_str = "\n\n---\n\n".join(context_texts)
        
        # ë²”ìš© ë³´ê³ ì„œ í˜•ì‹ í”„ë¡¬í”„íŠ¸
        prompt_text = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë¬¸ì„œ ì‘ì„±ìì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•œ ë³´ê³ ì„œ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨:**
1. ë°˜ë“œì‹œ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì°¸ê³  ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ì¤€, ë²•ë ¹ëª…, ê·œì¹™ëª…, ì œí’ˆëª…, ì„±ë¶„ëª… ë“±ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
3. ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
4. ë³´ê³ ì„œ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”.
5. ìˆ˜ì¹˜ë‚˜ ê¸°ì¤€ì´ ìˆëŠ” ê²½ìš° í‘œ í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.
6. ì‹œê°„ëŒ€ êµ¬ë¶„(ì£¼ê°„/ì•¼ê°„, ì˜¤ì „/ì˜¤í›„ ë“±)ì´ ìˆëŠ” ê²½ìš° ì´ëª¨ì§€(â˜€ï¸, ğŸŒ™ ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
7. ë„ë©”ì¸ì— ë§ëŠ” ì ì ˆí•œ ìš©ì–´ì™€ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ì•½í’ˆ, ì‹í’ˆ, í™˜ê²½, ê±´ì¶•, í–‰ì • ë“±).

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{context_str}

**ì§ˆë¬¸:**
{question}

**ë³´ê³ ì„œ ì‘ì„± í˜•ì‹:**

ë³´ê³ ì„œ (ì´ˆì•ˆ)
ì¼ì: {current_date}

ì§ˆë¬¸: {question}

info
í•µì‹¬ ë‹µë³€ ìš”ì•½

[ì°¸ê³  ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½. ë²•ë ¹ëª…, ê·œì¹™ëª…, ì œí’ˆëª…, ì„±ë¶„ëª… ë“±ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.]

[ìƒì„¸ ë‹µë³€ ë‚´ìš©]

1. [ì£¼ì œ 1 - ì°¸ê³  ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±]
[ì°¸ê³  ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±]
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê¸°ì¤€ì´ ìˆìœ¼ë©´ ëª…ì‹œ
- ë²•ë ¹ëª…, ê·œì¹™ëª…, ì œí’ˆëª…, ì„±ë¶„ëª… ë“±ì´ ìˆìœ¼ë©´ ì •í™•íˆ ì¸ìš©
- ì‹œê°„ëŒ€ë³„ ê¸°ì¤€ì´ ìˆëŠ” ê²½ìš° í‘œ í˜•ì‹ìœ¼ë¡œ ì œì‹œ:
  â˜€ï¸ ì£¼ê°„ (ì‹œê°„ëŒ€)
  [ìˆ˜ì¹˜] [ê¸°ì¡´ ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ í•¨ê»˜ í‘œì‹œ]
  ğŸŒ™ ì•¼ê°„ (ì‹œê°„ëŒ€)
  [ìˆ˜ì¹˜] [ê¸°ì¡´ ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ í•¨ê»˜ í‘œì‹œ]
- ì˜ì•½í’ˆ/ì‹í’ˆ ê´€ë ¨ ì •ë³´ì¸ ê²½ìš°: ì„±ë¶„, ìš©ëŸ‰, ì‚¬ìš©ë²•, ì£¼ì˜ì‚¬í•­ ë“±ì„ ëª…í™•íˆ ì œì‹œ
- í™˜ê²½ ê´€ë ¨ ì •ë³´ì¸ ê²½ìš°: ê¸°ì¤€ì¹˜, ì¸¡ì • ë°©ë²•, ê·œì œ ë‚´ìš© ë“±ì„ ëª…í™•íˆ ì œì‹œ

2. [ì£¼ì œ 2] (í•„ìš”í•œ ê²½ìš°)
[ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±]
- ì°¸ê³  ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”

3. ì°¸ê³  ì‚¬í•­
[ì°¸ê³  ë¬¸ì„œì—ì„œ ì¶”ê°€ë¡œ ì œê³µí•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ì •ë³´]
- ì˜ˆì™¸ ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì‚¬í•­
- ê´€ë ¨ ê¸°ê´€ì´ë‚˜ ì—°ë½ì²˜ ì •ë³´
- ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­

**ì¤‘ìš”: ì°¸ê³  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë“  ìˆ˜ì¹˜, ë²•ë ¹ëª…, ê·œì¹™ëª…, ì œí’ˆëª…, ì„±ë¶„ëª… ë“±ì€ ì°¸ê³  ë¬¸ì„œì—ì„œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.**
"""
        
        print(f"ì§ˆë¬¸: {question} ({len(document_ids)}ê°œ PDFì—ì„œ ê²€ìƒ‰)")
        
        # LLMì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
        response = self.llm.complete(prompt_text)
        answer = str(response).strip() if response else ""
        print(f"ë‹µë³€ ìƒì„± ì™„ë£Œ: ê¸¸ì´={len(answer)}")

        return answer

    def _search_chunks_with_pgvector(
        self,
        query_embedding: List[float],
        document_ids: List[str],
        similarity_top_k: int,
    ) -> List[Dict]:
        """
        pgvectorë¥¼ ì‚¬ìš©í•˜ì—¬ document_chunks í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰
        
        Supabase RPC í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ pgvector ê²€ìƒ‰ ìˆ˜í–‰
        """
        try:
            db = Database.get_client()
            
            print(f"pgvector ê²€ìƒ‰ ì‹œì‘: document_ids={document_ids}, similarity_top_k={similarity_top_k}")
            print(f"query_embedding ê¸¸ì´: {len(query_embedding)}")
            
            # RPC í•¨ìˆ˜ í˜¸ì¶œ (pgvector ê²€ìƒ‰)
            result = db.rpc(
                "search_document_chunks",
                {
                    "query_embedding": query_embedding,
                    "document_ids": document_ids,
                    "match_count": similarity_top_k * 2,  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°
                }
            ).execute()
            
            print(f"RPC í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼: {len(result.data) if result.data else 0}ê°œ ì²­í¬ ë°˜í™˜")
            
            if not result.data:
                print("ê²½ê³ : RPC í•¨ìˆ˜ê°€ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                # RPC í•¨ìˆ˜ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì§ì ‘ ì¿¼ë¦¬ë¡œ í´ë°±
                return self._search_chunks_fallback(query_embedding, document_ids, similarity_top_k)
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            chunks = []
            for row in result.data:
                chunks.append({
                    'id': row.get('id'),
                    'document_id': row.get('document_id'),
                    'content': row.get('content'),
                    'metadata': row.get('metadata', {}),
                    'score': row.get('similarity', 0.0),
                })
            
            print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ë°˜í™˜")
            return chunks
            
        except Exception as e:
            print(f"pgvector ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            print("RPC í•¨ìˆ˜ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ë°± ë°©ë²•ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            import traceback
            traceback.print_exc()
            # í´ë°±: ì§ì ‘ ì¿¼ë¦¬
            return self._search_chunks_fallback(query_embedding, document_ids, similarity_top_k)
    
    def _search_chunks_fallback(
        self,
        query_embedding: List[float],
        document_ids: List[str],
        similarity_top_k: int,
    ) -> List[Dict]:
        """
        RPC í•¨ìˆ˜ê°€ ì—†ì„ ë•Œ í´ë°±: ì§ì ‘ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        """
        try:
            db = Database.get_client()
            
            print("í´ë°±: ì§ì ‘ ì¿¼ë¦¬ë¡œ ì²­í¬ ê²€ìƒ‰")
            
            # document_chunks í…Œì´ë¸”ì—ì„œ í•´ë‹¹ document_idsì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
            all_chunks = (
                db.table("document_chunks")
                .select("*")
                .in_("document_id", document_ids)
                .execute()
            )
            
            if not all_chunks.data:
                print("í´ë°± ê²€ìƒ‰: ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            print(f"í´ë°± ê²€ìƒ‰: {len(all_chunks.data)}ê°œ ì²­í¬ ë°œê²¬, ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
            
            # ê° ì²­í¬ì˜ ì„ë² ë”©ê³¼ ì¿¼ë¦¬ ì„ë² ë”©ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            scored_chunks = []
            for chunk in all_chunks.data:
                if chunk.get('embedding'):
                    # ì„ë² ë”©ì´ ë¬¸ìì—´ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³€í™˜
                    chunk_embedding = chunk['embedding']
                    if isinstance(chunk_embedding, str):
                        import json
                        try:
                            chunk_embedding = json.loads(chunk_embedding)
                        except:
                            continue
                    
                    similarity = self._cosine_similarity(query_embedding, chunk_embedding)
                    scored_chunks.append({
                        'id': chunk['id'],
                        'document_id': chunk['document_id'],
                        'content': chunk['content'],
                        'metadata': chunk.get('metadata', {}),
                        'score': similarity,
                    })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            scored_chunks.sort(key=lambda x: x['score'], reverse=True)
            
            print(f"í´ë°± ê²€ìƒ‰ ì™„ë£Œ: {len(scored_chunks)}ê°œ ì²­í¬ ë°˜í™˜")
            return scored_chunks[:similarity_top_k * 2]
            
        except Exception as e:
            print(f"í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _cosine_similarity(self, vec1: List[float], vec2) -> float:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        
        vec2ëŠ” ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ
        """
        try:
            # vec2ê°€ ë¬¸ìì—´ì´ë©´ íŒŒì‹±
            if isinstance(vec2, str):
                import json
                vec2 = json.loads(vec2)
            
            # ë²¡í„° ê¸¸ì´ ê³„ì‚°
            import numpy as np
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception as e:
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def get_retrieved_nodes_from_documents(
        self,
        question: str,
        document_ids: List[str],
        similarity_top_k: int = 3,
    ) -> List:
        """
        ì—¬ëŸ¬ PDF ë¬¸ì„œë“¤ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë…¸ë“œ(ì²­í¬) ê²€ìƒ‰

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            document_ids: ê²€ìƒ‰í•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            similarity_top_k: ê° ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œ ìˆ˜

        Returns:
            ê²€ìƒ‰ëœ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬)
        """
        if not document_ids:
            return []

        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.embed_model.get_query_embedding(question)
        
        # pgvectorë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
        chunks = self._search_chunks_with_pgvector(
            query_embedding=query_embedding,
            document_ids=document_ids,
            similarity_top_k=similarity_top_k * 2,  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°
        )
        
        # LlamaIndex Node í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        from llama_index.core.schema import NodeWithScore, TextNode
        
        nodes = []
        for chunk in chunks:
            node = TextNode(
                text=chunk['content'],
                metadata={
                    'document_id': chunk['document_id'],
                    'pdf_name': chunk.get('metadata', {}).get('pdf_name', 'Unknown'),
                    **chunk.get('metadata', {}),
                }
            )
            node_with_score = NodeWithScore(
                node=node,
                score=chunk.get('score', 0.0),
            )
            nodes.append(node_with_score)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        nodes.sort(key=lambda x: x.score or 0, reverse=True)
        
        return nodes[:similarity_top_k]
